{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1313bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1413af",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = pd.read_csv(\"links.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43c03e8",
   "metadata": {},
   "source": [
    "### Link Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233da69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"D:\\AIMT course\\Term 2\\python\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url = \"https://www.kijiji.ca/b-real-estate/city-of-toronto/c34l1700273?ll=43.653226%2C-79.383184&address=Toronto%2C+ON&radius=304.0\"\n",
    "\n",
    "def scrape_links():\n",
    "    link_elements = driver.find_elements('css selector', 'div.info-container div.title a.title')\n",
    "    links = [link.get_attribute('href') for link in link_elements]\n",
    "    return links\n",
    "\n",
    "all_links = []\n",
    "\n",
    "def click_next():\n",
    "    try:\n",
    "        next_button = driver.find_element('xpath', '//div[@class=\"bottom-bar\"]//a[@title=\"Next\"]')\n",
    "        next_button.click()\n",
    "    except NoSuchElementException:\n",
    "        return False\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "for i in range(600):\n",
    "    current_links = scrape_links()\n",
    "    all_links.extend(current_links)\n",
    "    next_page_exists = click_next()\n",
    "\n",
    "\n",
    "links = pd.DataFrame(all_links)\n",
    "links.to_csv(\"links.csv\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea0c9b4",
   "metadata": {},
   "source": [
    "### Details Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a1519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "\n",
    "for url in link[\"0\"]:\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        \n",
    "        try:\n",
    "            show_more_button = WebDriverWait(driver, 3).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//button[contains(@class, 'showMoreButton-')]\"))\n",
    "            )\n",
    "\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", show_more_button)\n",
    "            driver.execute_script(\"arguments[0].click();\", show_more_button)\n",
    "            time.sleep(2)\n",
    "\n",
    "        except TimeoutException:\n",
    "            continue\n",
    "        \n",
    "        page_source = driver.page_source\n",
    "\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        title = soup.find('h1', class_=re.compile(r'title-\\d+')).text.strip() if soup.find('h1', class_=re.compile(r'title-\\d+')) else np.nan\n",
    "        address = soup.find('span', {'itemprop': 'address'}).text.strip() if soup.find('span', {'itemprop': 'address'}) else np.nan\n",
    "\n",
    "        price_value = []\n",
    "        price_label = []\n",
    "        price_elements = soup.find_all('div', class_=re.compile(r'priceWrapper-\\d+'))\n",
    "        for price_element in price_elements:\n",
    "            price_value.append(price_element.find('span').text.strip())\n",
    "            price_label.append(price_element.find('span', class_=re.compile(r'utilities-\\d+')).text.strip())\n",
    "\n",
    "\n",
    "        date_posted_pattern = re.compile(r'\\d+ days ago')\n",
    "        date_posted_element = soup.find(text=date_posted_pattern)\n",
    "        date_posted = date_posted_pattern.search(date_posted_element).group(0) if date_posted_element else np.nan\n",
    "\n",
    "        unit_row = soup.find('div', class_=re.compile(r'unitRow-\\d+')).text.strip() if soup.find('div', class_=re.compile(r'unitRow-\\d+')) else np.nan\n",
    "\n",
    "        parking_pattern = re.compile(r'Parking Included:\\s*(\\d+)')\n",
    "        parking_element = soup.find(text=parking_pattern)\n",
    "        parking = parking_pattern.search(parking_element).group(1) if parking_element else np.nan\n",
    "\n",
    "        agreement_type_pattern = re.compile(r'Agreement Type:\\s*(.+)')\n",
    "        agreement_type_element = soup.find(text=agreement_type_pattern)\n",
    "        agreement_type = agreement_type_pattern.search(agreement_type_element).group(1) if agreement_type_element else np.nan\n",
    "\n",
    "        air_conditioning_pattern = re.compile(r'Air Conditioning:\\s*(.+)')\n",
    "        air_conditioning_element = soup.find(text=air_conditioning_pattern)\n",
    "        air_conditioning = air_conditioning_pattern.search(air_conditioning_element).group(1) if air_conditioning_element else np.nan\n",
    "\n",
    "        description = soup.find('div', class_=re.compile(r'descriptionContainer-\\d+')).text.strip()\n",
    "    \n",
    "        visit_element = soup.find('div', class_=re.compile(r'visitCounter-\\d+'))\n",
    "        visit_count = visit_element.find('span').text.strip() if visit_element else np.nan\n",
    "\n",
    "\n",
    "        data = {\n",
    "            'Title': title,\n",
    "            'Address': address,\n",
    "            'Price Label': price_label,\n",
    "            'Price Value': price_value,\n",
    "            'Date Posted': date_posted,\n",
    "            'Unit Row': unit_row,\n",
    "            'Parking': parking,\n",
    "            'Agreement Type': agreement_type,\n",
    "            'Air Conditioning': air_conditioning,\n",
    "            'Description': description,\n",
    "            'Visits': visit_count\n",
    "        }\n",
    "\n",
    "        data_list.append(data)\n",
    "    except TimeoutException:\n",
    "        print(\"Timeout while loading the page. Skipping URL:\", url)\n",
    "        continue\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "df.to_csv(\"final-data-kijiji.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f751e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
